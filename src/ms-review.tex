%                                                                 aa.dem
% AA vers. 9.0, LaTeX class for Astronomy & Astrophysics
% demonstration file
%                                                       (c) EDP Sciences
%-----------------------------------------------------------------------
%
%\documentclass[referee]{aa} % for a referee version
%\documentclass[onecolumn]{aa} % for a paper on 1 column
%\documentclass[longauth]{aa} % for the long lists of affiliations
%\documentclass[rnote]{aa} % for the research notes
%\documentclass[letter]{aa} % for the letters
%\documentclass[bibyear]{aa} % if the references are not structured
%                              according to the author-year natbib style

% \documentclass[]{aa}
\documentclass[longauth]{aa}

\usepackage{graphicx}
\usepackage{url}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{import}
\usepackage{fancyvrb}
\usepackage{color}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\usepackage[autostyle]{csquotes}

%% For the review, add the line numbers
\usepackage[switch]{lineno}
\linenumbers

\input{code-examples/minted.tex}
\newcommand{\todo}[1]{\textcolor{red}{TODO: #1}\PackageWarning{TODO:}{#1!}}

\newcommand{\code}[1]{\texttt{#1}}


\begin{document}
\newcommand{\PythonUrl}{\url{http://fits.gsfc.nasa.gov/}\xspace}
\newcommand{\FitsUrl}{\url{http://fits.gsfc.nasa.gov/}\xspace}
\newcommand{\GammapyUrl}{\url{http://gammapy.org}\xspace}
\newcommand{\GadfUrl}{\url{http://gamma-astro-data-formats.readthedocs.io/}\xspace}
\newcommand{\ReadthedocsUrl}{\url{https://readthedocs.org/}\xspace}
\newcommand{\TravisUrl}{\url{https://www.travis-ci.org/}\xspace}

\newcommand{\NaimaUrl}{\url{https://github.com/zblz/naima}\xspace}

% Note: not sure if we want to use that ... doesn't look too pretty
\newcommand{\astropy}{Astropy\xspace}
\newcommand{\gammapy}{Gammapy\xspace}
\newcommand{\scipy}{Scipy\xspace}
\newcommand{\numpy}{Numpy\xspace}
\newcommand{\iminuit}{IMinuit\xspace}
\newcommand{\sherpa}{Sherpa\xspace}
\newcommand{\agnpy}{Agnpy\xspace}
\newcommand{\matplotlib}{Matplotlib\xspace}


\newcommand{\hess}{H.E.S.S.\xspace}
\newcommand{\hawc}{HAWC\xspace}
\newcommand{\veritas}{VERITAS\xspace}
\newcommand{\magic}{MAGIC\xspace}
\newcommand{\astri}{ASTRI\xspace}
\newcommand{\iact}{IACT\xspace}
\newcommand{\iacts}{IACTs\xspace}
\newcommand{\cta}{CTA\xspace}
\newcommand{\swgo}{SWGO\xspace}
\newcommand{\irf}{IRF\xspace}
\newcommand{\irfs}{IRFs\xspace}
\newcommand{\fermi}{\textit{Fermi}-LAT\xspace}
\newcommand{\gammaray}{$\gamma$-ray\xspace}
\newcommand{\gammarays}{$\gamma$ rays\xspace}
\newcommand{\gadf}{GADF\xspace}
\newcommand{\milagro}{MILAGRO\xspace}
\newcommand{\github}{GitHub\xspace}


% Front matter
\title{Gammapy: A Python package for gamma-ray astronomy}
\titlerunning{The Gammapy library, v1.0}
\authorrunning{The Gammapy project}

\author{
	{\it Paper Authors} \and
    Axel \and
	Régis \and
	Quentin \and
	Atreyee \and
	Cosimo \and
	Fabio \and
	Bruno \and
	Laura \and
	Jose Enrique \and
	\\
	{\it Coordination Committee}\thanks{Corresponding author:
\href{mailto:GAMMAPY-COORDINATION-L@IN2P3.FR}{GAMMAPY-COORDINATION-L@IN2P3.FR}} \and
	Fabio Acéro \and
	David Berge \and
	Catherine Boisson \and
	Jose Louis Contreras \and
	Axel Donath \and
	Stefan Funk \and
	Christopher van Eldik \and
	Matthias Fue{\ss}ling \and
	Jim Hinton \and
	Bruno Khélifi \and
	Rub{\'e}n L{\'o}pez-Coto \and
	Fabio Pintore \and
	Régis Terrier \and
	Roberta Zanin \and
    \\
	{\it Gammapy Project Contributors} \and
	Dark Vador \inst{\ref{inst:0}} \and
	Unknown Contributor \inst{\ref{inst:unknown}}
}



\institute{
	Fake institute, Vador city, Moon \label{inst:0} \and
	unknown \label{inst:unknown}
}

% \abstract{}{}{}{}{}
% 5 {} token are mandatory

\abstract
	{
		Traditionally, \gammaray astronomy has been conducted
		by experiments employing proprietary data and analysis software.
		However, the next generation of \gammaray instruments,
		such as the the Cherenkov Telescope Array, will be operated as open observatories.
		Alongside the data, they will make software tools for their analysis available to the community.
    This necessity prompted the development of open, high-level astronomy software customised for high energy astrophysics.
	}
	{
		In this article, we present \gammapy, an open-source Python package for the analysis of astronomical \gammaray data,
		and illustrate the functionalities of its first long-term-support release, version 1.0.
		Built on the modern Python scientific ecosystem, \gammapy provides a uniform platform for reducing and
		modelling data from different \gammaray instruments for many analysis scenarios.
    \gammapy complies with several well-established data conventions in high-energy astrophysics, providing serialised data products that
		are interoperable with other software packages.
	}
	{
		Starting from event list and instrument response functions,
		\gammapy provides the functionalities for reducing data binned in energy and sky coordinates.
		To handle the residual hadronic background, several techniques for background estimation
		are implemented in the package.
		After the data are binned, the flux and morphology of one or more \gammaray sources can be estimated
		using Poisson maximum likelihood fitting
		and assuming a variety of spectral, temporal and spatial models.
		Estimation of flux points, likelihood profiles and light curves is also supported.
	}
	{
		After describing the structure of the package, we show the capabilities of \gammapy
		in multiple traditional and novel \gammaray analysis scenarios using public data
		such as spectral and spectro-morphological modelling and estimations of a spectral energy
		distribution and a light curve.
		Its flexibility and its power are displayed in a final multi-instrument example,
		where datasets from different instruments, at different stages of data reduction,
		are simultaneously fitted with an astrophysical flux model.
	}{}


% \date{Received September 15, 1996; accepted March 16, 1997}
\keywords{
	Gamma rays: general -
	Astronomical instrumentation, methods and techniques -
	Methods: data analysis
}

\maketitle

% Main part
\section{Introduction}
\label{sec:introduction}


%% \gammaray astronomy
%% CN: I think we should use the expression Water Cherenkov Detectors and use the
%% abbreviation (WCD) all along the text, later we use "particle samplers"
The \gammaray range of the electromagnetic spectrum provides us insights into the
most energetic processes in the universe such as those accelerating particles in the surroundings of
black holes, and remnants of supernova explosions. As in other
branches of astronomy, \gammarays can be observed by both
satellite as well as ground based instruments.
Ground-based instruments use the Earth's atmosphere as a particle detector.
Very-high-energy (VHE) cosmic \gammarays interact in the atmosphere and
create a large shower of secondary particles that can be observed from the ground.
Ground-based \gammaray astronomy relies on these extensive air showers to detect the
primary \gammaray photons and infer their incident direction and energy.
VHE \gammaray astronomy covers the energy range from fews tens of ${\rm GeV}$ up to the ${\rm PeV}$.
There are two main categories of ground-based instruments: 

\textit{Imaging Atmospheric Cherenkov Telescopes (IACT)} obtain images of the atmospheric showers
by detecting the Cherenkov radiation emitted by the cascading charged particles and
use these images to reconstruct the properties of the incident particle.
Those instruments have a limited field of view (FoV) and duty cycle, but
good energy and angular resolution.
	
\textit{Water Cherenkov Detectors (WCD)} detect particles directly from the tail of the
shower when it reaches the ground. These instruments have a very
large FoV, large duty-cycle but higher energy threshold and
usually have lower signal to noise ratios compared to IACTs~\citep{2015CRPhy..16..610D}.


\begin{figure*}[t]
	\centering
	\includegraphics[width=\textwidth]{figures/big-picture.pdf}
	\caption{
		Core idea and relation of \gammapy to different \gammaray instruments
		and the gamma astro data formats (GADF). The top left shows the
		group of current and future pointing instruments based on the 
		imaging atmospheric Cherenkov technique (IACT). This includes
		instruments such as the Cherenkov Telescope Array (CTA),
		the High Energy Stereoscopic System (H.E.S.S.), the
		Major Atmospheric Gamma Imaging Cherenkov Telescopes (MAGIC),
		and the Very Energetic Radiation Imaging Telescope Array System (VERITAS).
		The lower left shows the group of all-sky instruments such as the
		Fermi Large Area Telescope (Fermi-LAT) and the High Altitude
		Water Cherenkov Observatory (HAWC). The calibrated data of all those
		instruments can be converted and stored into the common GADF data format.
		\gammapy can read data stored in the GADF format.
		The \gammapy package is not a part of any instrument, but instead
		provides a common interface to the data and analysis of all
		these \gammaray instruments. This way users can also easily combine data from
		different instruments and perform a joint analysis.
		\gammapy is built on the scientific Python ecosystem, and the required dependencies
		are shown below the \gammapy logo.
	}
	\label{fig:big_picture}

\end{figure*}


%% Context
Ground-based \gammaray astronomy has been historically conducted
by experiments operated by independent collaborations, each relying
on their own proprietary data and analysis software developed as part of the
instrument. While this model has been successful so far, it does not
permit easy combination of data from several instruments and therefore
limits the interoperability of existing facilities. This lack of
interoperability currently limits the full exploitation of the
available \gammaray data, especially because the different instruments often have
complementary sky coverages, and the various detection
techniques have complementary properties in terms of the energy range covered,
duty cycle and spatial resolution.

%TODO: better transition into context
The Cherenkov Telescope Array (CTA) will be the first ground-based \gammaray instrument to be operated as an open observatory.
Its high-level data\footnote{The lowest reduction level of data published by CTAO will be reconstructed event lists and corresponding instrument response functions.} will be shared publicly after
some proprietary period, and the software required to analyze it will be distributed
as well. To allow the re-usability of existing instruments and their interoperability,
it is required to use open data formats and open tools that can support the various analysis methods
commonly used in the field.

%% Context : formats
In practice, the data reduction workflow of all \gammaray observatories
is remarkably similar. After data calibration, shower events are reconstructed and
gamma/hadron separation is applied to build lists of \gammaray-like events.
The lists of \gammaray events are then used to derive scientific results, such as spectra, sky maps
or light curves, taking into account the observation specific instrument response functions (IRF).
Once the data is reduced to a list of events with reconstructed physical properties of the primary particle,
the information is independent of the data-reduction process, and, eventually, of the detection technique. This implies,
for example, that high-level data from IACTs and WCDs can be represented
with the same data model.
The efforts to create a common format usable by various instruments
converged in the so-called \textit{Data Formats for \gammaray Astronomy}
initiative~\citep{gadf_proc,gadf_universe}, abbreviated to
\texttt{gamma-astro-data-formats} (\gadf). This proposes prototypical
specifications to produce files based on the flexible image transport system
(FITS) format~\citep{fits} encapsulating this high-level information. This is
realized by storing a list of \gammaray-like events with their reconstructed and observed
quantities such as energy, incident direction and arrival time and a parametrisation of
the IRFs associated with the event list data. %% where? in the data units of a FITS file.

%% Context: development of open software
In the past decade observing the \gammaray sky has transitioned from a niche in the field of
particle physics to an established branch of astronomy, completing the view of the
sky in high energies. At the same time \textit{Python}
has become extremely popular as a scientific programming language,
in particular in the field of data sciences. This success is
mostly attributed to the simple and easy to learn syntax, the ability to act as
a \enquote{glue} language between different programming languages and last but not least
the rich ecosystem of packages and its open and supportive community \citep{Momcheva2015}.

In the sub-field of astronomy, it was the \astropy project \citep{astropy} that 
was created in 2012 to build a community-developed core Python package for astronomy.
It offers basic functionalities that astronomers of many fields need, such as representing
and transforming astronomical coordinates, manipulating physical quantities including units
as well as reading and writing FITS files.


%% \gammapy: concept and goals
%%% Question: starting date? Is is relevant, what is the real start?
The \gammapy project was started following the idea of \astropy with the objective of building a common
software library for very high-energy \gammaray data analysis \citep{gammapy_2015}. 
The core of the idea is illustrated in Figure~\ref{fig:big_picture}. Various \gammaray instruments
export their data to a standardised common data format -- the \gadf. This data can then be combined and
analysed using a single common software library.
This means that the \gammapy package is not specific to any instrument,
but an independent community developed software project.
The \gammapy package is is built on the scientific Python ecosystem: it uses \numpy~\citep{numpy} for n-dimensional data
structures, \scipy~\citep{2020SciPy-NMeth} for numerical algorithms, \astropy~\citep{astropy} for
astronomy-specific functionality, and \matplotlib~\citep{matplotlib} for visualization.

%% CN: what about mentioning the joint Crab? or is it mentioned later?
With the public availability of the \gadf format sepcifications and the
\gammapy package, some experiments started to make limited subsets of
their \gammaray data publicly available for testing and validating
\gammapy. For example, the \hess collaboration released a limited test
dataset (about 50 hours of observations taken between 2004 and 2008)
based on the \gadf DL3 format \citep{HESS_DR1}. This data release served
as a basis for validation of open analysis tools, including \gammapy 
\cite[see e.g.][]{Mohrmann2019}. The \hawc collaboration also released
a limited test dataset of the Crab Nebula, which was used to validate
the \gammapy package in \cite{Olivera2022}.

%% Paper outline
In this article, we describe the general structure of the \gammapy package,
its main concepts and organisational structure. We start in
Section~\ref{sec:gammaray-data-analysis} with a general overview
of the data analysis workflow in very high-energy \gammaray astronomy. Then we
show how this workflow is reflected in the structure of the \gammapy package 
in Section~\ref{sec:gammapy-package}, while also
describing the various subpackages it contains. Section~\ref{sec:applications}
presents a number of applications, while Section~\ref{sec:gammapy-project}
finally discusses the project organization.


\section{Gamma-ray Data Analysis}
\begin{figure*}[h!]
	\centering
	\includegraphics[width=1.\textwidth]{figures/data_flow.pdf}
	\caption{
		\gammapy sub-package structure and data analysis workflow. The top row
        defines the different levels of data reduction, from lists of \gammaray-like
        events on the left (DL3), to high-level scientific products products
        (DL5) on the right. The direction of the data flow is illustrated with the
        gray arrows. The gray folder icons represent the different sub-packages
        in \gammapy and their names. Below each icon there is a list of the most
        important objects defined in the sub-package.
    }
	\label{fig:data_flow}
\end{figure*}
%
\label{sec:gammaray-data-analysis}
% This might not be the bast place compared to introduction
The data analysis process in \gammaray astronomy is usually split into two parts.
The first one deals with the data processing from camera measurement, calibration, event
reconstruction and selection to yield a list of reconstructed \gammaray event candidates.
This part of the data reduction sequence, sometimes referred to as low-level analysis,
is usually very specific to a given observation technique and even to a given instrument.

The other sequence, referred to as high-level analysis, deals with the extraction of physical
quantities related to \gammaray sources and the production of high-level products such as spectra,
light curves and catalogs. The methods applied here are more generic and are broadly
shared across the field. The similarity in the high-level analysis would also allow
for combining data from multiple instruments, but could not be fully exploited, due to
a lack of common data formats and software tools.

%\subsection{Predicted counts and Instrument Response Functions}

To extract physically relevant information, such as the flux, spatial or spectral shape of one or more sources,
an analytical model is commonly adopted to describe the intensity of gamma-ray sources as a function of the energy,
$E_{\rm true}$, and of the position in the field of view, $p_{\rm true}$:
\begin{equation}
    \Phi(p_{\rm true}, E_{\rm true}, \hat{\theta}) \quad \mathrm{[TeV^{-1}cm^{-2}s^{-1}]}
    \label{eq:general_source_model}
\end{equation}
where $\hat{\theta}$ is a set of model parameters that can be adjusted in a fit. To convert this analytical flux model
into a prediction on the number of gamma-ray events, $N_{\rm pred}$, with their estimated energy $E$ and position $p$, the model
is convolved through the response function of the instrument.

%% CN: not sure if we already commented about this, but align is artificially
%% numbering every new line as a new equation
%% I think
%% \begin{equation}
%%   \begin{split}
%%     ...
%%   \begin{split}
%% \begin{equation}
%% will avoid to number each new line

In the most general way, we can write the expected number of detected events from the sky model $\Phi$ at measured position $p$ and energy $E$, for
a given set of parameters $\hat{\theta}$, as:
%
\begin{equation}
	\begin{split}
   		N(p, E, \hat{\theta})\,{\rm d}p\,{\rm d}E = &t_{\rm obs} \int_{E_{\rm true}} \int_{p_{\rm true}}  R(p, E|p_{\rm true}, E_{\rm true}) \\
   		&\cdot \Phi(p_{\rm true}, E_{\rm true}, \hat{\theta} ) {\rm d}E_{\rm true} {\rm d}p_{\rm true}
	\end{split}
\end{equation}

where $R(p, E| p_{\rm true}, E_{\rm true})$ is the instrument response and $t_{\rm obs}$ is the observation time

A common assumption is that the instrument response can be simplified as the product
of three independent functions:

\begin{equation}
	\begin{split}
   R(p, E|p_{\rm true}, E_{\rm true}) = &~A_{\rm eff}(p_{\rm true}, E_{\rm true}) \\
	& \cdot PSF(p|p_{\rm true}, E_{\rm true})\\
    & \cdot E_{\rm disp}(E|p_{\rm true}, E_{\rm true})
	\end{split}
\end{equation}

where:
\begin{itemize}
\setlength\itemsep{1em}
\item $A_{\rm eff}(p_{\rm true}, E_{\rm true})$ is the effective collection area of the detector. It is the product
  of the detector collection area times its detection efficiency at true energy $E_{\rm true}$ and position $p_{\rm true}$.
\item $PSF(p|p_{\rm true}, E_{\rm true})$ is the point spread function (PSF). It gives the probability of
  measuring a direction $p$ when the true direction is $p_{\rm true}$ and the true energy is $E_{\rm true}$.
  \gammaray instruments consider the probability density of the angular separation between true and reconstructed directions
  $\delta p = p_{\rm true} - p$, i.e. $PSF(\delta p|p_{\rm true}, E_{\rm true})$.
\item $E_{\rm disp}(E|p_{\rm true}, E_{\rm true})$ is the energy dispersion. It gives the probability to
  reconstruct the photon at energy $E$ when the true energy is $E_{\rm true}$ and the true position $p_{\rm true}$.
  \gammaray instruments consider the probability density of the migration $\mu=\frac{E}{E_{\rm true}}$,
  i.e. $E_{\rm disp}(\mu|p_{\rm true}, E_{\rm true})$.
\end{itemize}

%% CN: except for the figures we never introduced what the DL3 or the Data Level acronym is
%% maybe add (see Fig.~1)
\gammaray data at the Data Level 3 (DL3) therefore consist of lists of \gammaray-like events and their
corresponding instrument response functions. The latter include the effective area ($A_{\rm eff}$),
point spread function and energy dispersion ($E_{\rm disp}$).
In general, they depend on event's detector
geometrical parameters, e.g. the field-of-view location or the event elevation angle. So they might be parametrised as
function of such parameters specific to the instrumental technical.

%% CN: this paragraph repeats what is commented in the previous formula and itemize and can be removed

An additional component of DL3 IRFs is the residual hadronic background model $Bkg$.
It represents the intensity of charged particles misidentified as \gammarays that are expected
during an observation. It is defined as a function of the measured position in the field-of-view
and measured energy.

In total, the expected number of events in a \gammaray observation is given by:
\begin{equation}
	\begin{split}
  N(p, E, \hat{\theta})\ {\rm d}p\ {\rm d}E =  &E_{\rm disp} \circledast \left[ PSF \circledast \left( A_{\rm eff} \cdot t_{\rm obs} \cdot \Phi(\hat{\theta}) \right) \right]\\
                       & + Bkg(p, E) \cdot t_{\rm obs}
	\end{split}
\end{equation}
				
%% CN: Instead of rewriting this formula and introducing this new notation for the convolution,
%% can't we just say that, when predicting counts, $Bkg(p, E)$ has to be summed to the convolution
%% (or integral) in the previous equation?

Finally, predicted and observed events, $N_{obs}$, can be then combined in a likelihood function,
$\mathcal{L}(\hat{\theta}, N_{obs})$, usually Poissonian, that is maximised to obtain the best-fit parameters of the flux model, $\hat{\theta}$.

\subsection{Data analysis workflow}
The first step in \gammaray data analysis is the selection and extraction of observations
based of their metadata including information such as pointing direction, observation
time and observation conditions. The access to the events data and instrument 
reponse per observation is supported by classes and methods
in the \code{gammapy.data} (see Section~\ref{ssec:gammapy-data}) and the \code{gammapy.irf}
(see Section~\ref{ssec:gammapy-irf}) subpackages.

The next step of the analysis is the data reduction, where all observation events and instrument
responses are filled into or projected onto a common physical coordinate system, defined by
a map geometry. The definition of the map geometry typically consists of a spectral dimension
defined by a binned energy axis and of spatial dimensions, which either define 
a spherical projection from celestial coordinates to a pixelised image space
or a single region on the sky. The \code{gammapy.maps} subpackage provides
general multidimensional geometry objects and the associated data structures
(see Section~\ref{ssec:gammapy-maps}).

%% CN: I would actually distingusih two cases here:
%% 1) the Bkg is already provided as a parametrisation
%% 2) the Bkg is not provided and, thus, has to be estimated from the obs.
After all data has been projected into the same geometry, it is typically
required to improve the residual hadronic background estimate. As residual hadronic
background models can be subject to significant systematic uncertainties,
these models can be improved by taking into account actual data
from regions without known \gammaray sources. This includes methods 
such as the ring or the field-of-view background techniques or
background measurements performed within, e.g. reflected regions~\citep{Berge07}.
Data measured at the field-of-view or energy boundaries of the instrument are typically
associated with a systematic uncertainty in the IRF. For this reason this part 
of the data is often excluded from subsequent analysis by defining regions of
 \enquote{safe} data in the spatial as well as energy dimension.
All of these data reduction steps are performed by classes and functions
implemented in the \code{gammapy.makers} subpackage (see Section~\ref{ssec:gammapy-makers}).

%% CN: for 1D analysis datasets, DL4 can actually be written as OGIP files,
%% so as reduced datasets in a general format, not specific to Gammapy.
%% Maybe you can briefly mention that this is due to the fact that the 1D Poisson
%% spectral analysis is liong consilidated, but this also means that Gammapy 's
%% dataset can handle lower energy data, e.g. X-ray.
%% We also said in the abstract that Gammapy is compliant with other specifications
%% adopted in high-energy astrophysics.
The counts data and the reduced IRFs in the form of maps are bundled into datasets
that represent the fourth data level (DL4). These reduced datasets can be written to disk,
in a format specific to \gammapy to allow users to read them back at any time later
for modeling and fitting. Different variations of such datasets support different 
analysis methods and fit statistics. The datasets can be used to perform a joint-likelihood
fit, allowing one to combine different measurements, e.g. from different observations
but also from different instruments or event classes. They can also be used for binned
simulation as well as event sampling to simulate DL3 events data.
The various DL4 objects and the associated functionalities are
implemented in the \code{gammapy.datasets} subpackage (see Section~\ref{ssec:gammapy-datasets}).

The next step is then typically to model and fit the datasets, either
individually, or in a joint likelihood analysis. For this purpose \gammapy
provides a uniform interface to multiple fitting backends. In addition to
providing a variety of built-in models, including spectral,
spatial and temporal model classes to describe the \gammaray emission in the sky,
custom user-defined models are also supported.
Spectral models can be simple analytical models or more complex ones from radiation
mechanisms of accelerated particle populations (e.g. inverse Compton or $\pi^{o}$ decay).
Independently or subsequently to the global modelling, the data can be
re-grouped to compute flux points, light curves and flux as well as significance
maps in different energy bands.
The modelling and fitting functionalities are implemented in the \code{gammapy.modeling},
\code{gammapy.estimators} and \code{gammapy.stats} subpackages (see respectively
Section~\ref{ssec:gammapy-modeling}, \ref{ssec:gammapy-estimators} and \ref{ssec:gammapy-stats}).

\section{\gammapy Package}
\label{sec:gammapy-package}
\subsection{Overview}
\label{ssec:overview}
%
%
The \gammapy package is structured into multiple sub-packages. The definition
of the content of the different sub-packages follows mostly the stages of the
data reduction workflow described in the previous section. Sub-packages
either contain structures representing data at different reduction
levels or algorithms to transition between these different levels.

Figure~\ref{fig:data_flow} shows an overview of the different sub-packages and
their relation to each other. The \code{gammapy.data} and \code{gammapy.irf}
sub-packages define data objects to represent DL3 data, such as
event lists and IRFs as well as functionality
to read the DL3 data from disk into memory. The \code{gammapy.makers} sub-package
contains the functionality to reduce the DL3 data to binned maps.
Binned maps and datasets, which represent a collection of binned
maps, are defined in the \code{gammapy.maps} and \code{gammapy.datasets}
sub-packages, respectively. Parametric models, which are defined in
\code{gammapy.modeling}, are used to jointly model a combination
of datasets, for example, to make spectum using data from several facilities. Estimator classes,
which are contained in \code{gammapy.estimators}, are used to
compute higher level science products such as flux and signficance maps,
light curves or flux points. Finally there is a \code{gammapy.analysis}
sub-package which provides a high-level interface for executing analyses
defined from configuration files. In the following sections we will
introduce all sub-packages and their functionalities in more detail.


\subsection{gammapy.data}
\label{ssec:gammapy-data}
The \code{gammapy.data} sub-package implements the functionality to select,
read, and represent DL3 \gammaray data in memory. It provides the main user
interface to access the lowest data level. \gammapy currently only
supports data that is compliant with \code{v0.2} and \code{v0.3} of the \gadf data format.
DL3 data are typically bundled into individual observations, which
corresponds to stable periods of data acquisition. For IACT data analysis,
for which the GADF data model and Gammapy were initially conceived,
these are usually $20 - 30\,{\rm min}$ long.
Each observation is assigned a unique integer ID for reference.

A typical usage example is shown in Figure~\ref{fig*:minted:gp_data}.
First a \code{DataStore} object is created from the path of the data
directory. The directory contains an observation as well as FITS HDU 
index file which assigns the correct data and IRF FITS files and HDUs
to the given observation ID. The \code{DataStore}
object gathers a collection of observations and provides ancillary
files containing information about the telescope observation mode and the
content of the data unit of each file. The \code{DataStore} allows for
selecting a list of observations based on specific filters.

The DL3 level data represented by the \code{Observation} class consist
of two types of elements: first, a list of \gammaray events with relevant physical
quantities such as estimated energy, direction and arrival
times, which is represented by the \code{EventList} class. Second, a set of
associated IRFs, providing the response of the system, typically
factorised in independent components as described in
Section~\ref{ssec:gammapy-irf}. The separate handling of event lists and IRFs
additionally allows for data from non-IACT \gammaray instruments to be read. For
example, to read \fermi data, the user can read separately their event list
(already compliant with the \gadf specifications) and then find the appropriate
IRF classes representing the response functions provided by \fermi, see
example in Section~\ref{ssec:multi-instrument-analysis}.

%% CN: Shouldn't we display the output of this snippet?
%% otherwise what's the point of the `print`' functions?
\begin{figure}
	\small
	\import{code-examples/generated/}{gp_data}
	\caption{
        Using \code{gammapy.data} to access DL3 level data with a \code{DataStore} object.
        Individual observations can be accessed by their unique integer observation id number.
        The actual events and instrument response functions can be accessed
        as attributes on the \code{Observation} object, such as \code{.events}
        or \code{.aeff} for the effective area information. The output
		of the code example is shown in Figure~\ref{fig:code_example_gp_data}.
    }
	\label{fig*:minted:gp_data}
\end{figure}
%

\subsection{gammapy.irf}
\label{ssec:gammapy-irf}
%
%
\begin{figure*}[ht!]
	\centering
	\includegraphics[width=1.\textwidth]{figures/irfs.pdf}
	\caption{
		Using \code{gammapy.irf} to read and plot instrument response functions.
		The left panel shows the effective area as a function of energy for
		the \cta, \hess, \magic, \hawc and \fermi instruments. The right panel shows
		the $68\%$ containment radius of the PSF as a function of energy for the \cta, \hess
		and \fermi instruments. The \cta IRFs are from the \textit{prod5} production. The \hess IRFs are from the DL3 DR1,
        using observation ID 033787. The \magic effective area is computed for a
        $20\,{\rm min}$ observation at the Crab Nebula coordinates. The
		\fermi IRFs use \textit{pass8} data and are also taken at the position of the Crab Nebula.
		The \hawc effective area is shown for the event classes $N_{Hit}=5 - 9$ as light gray
		lines along with the sum of all event classes as a black line. The \hawc IRFs are taken from
		the first public release of event data the \hawc collaboration. All IRFs do not correspond
		to the latest performance of the instruments, but still are representative of the 
		detector type and energy range. We also exclusively relied on publicly available
		data provided by the collaborations. The data is also availabe in the
		\code{gammapy-data} repository.
    }
	\label{fig:irfs}
\end{figure*}
%

The \code{gammapy.irf} sub-package contains all classes and functionality
to handle IRFs in a variety of formats.
Usually, \irfs store instrument properties in the form of multi-dimensional
tables, with quantities expressed in terms of energy (true or reconstructed),
off-axis angles or cartesian detector coordinates. The main information stored in
the common \gammaray \irfs are the effective area, energy dispersion,
point spread function and background rate. The \code{gammapy.irf}
sub-package can open and access specific \irf extensions,
interpolate and evaluate the quantities of interest on both energy and spatial
axes, convert their format or units, plot or write them into
output files. In the following, we list the main classes of the
sub-package:

\subsubsection{Effective Area}
\gammapy provides the class \code{EffectiveAreaTable2D} to
manage the effective area, which is usually defined in terms of true energy and offset angle.
The class functionalities offer the possibility to read from files or to create
it from scratch. The \code{EffectiveAreaTable2D} class can also convert, interpolate,
write, and evaluate the effective area for a given energy and offset angles, or
even plot the multi-dimensional effective area table.

%% CN: ref. for King function https://ui.adsabs.harvard.edu/abs/2011A%26A...534A..34R/abstract
%% CN: I would not say that the PSFKing takes into account "the gamma and sigma", fo what?
%% I would put a formula or quickly describe the function (e.g. it is a power-law, an exponential...)
\subsubsection{Point Spread Function}
\gammapy allows user to treat different kinds of PSFs,
in particular, parametric multi-dimensional Gaussians (\code{EnergyDependentMultiGaussPSF})
or King profile functions (\code{PSFKing}). The \code{EnergyDependentMultiGaussPSF}
class is able to handle up to three
Gaussians, defined in terms of amplitudes and sigma given for each true energy
and offset angle bin. Similarly, \code{PSFKing} takes into account the gamma and
sigma parameters. The general \code{ParametricPSF} class allows users to create a
custom PSF with a parametric representation different from Gaussian(s) or King profile(s).
The generic \code{PSF3D} class stores a radial symmetric profile of a
PSF to represent non-parametric shapes, again depending on true energy
of offset form the pointing position.

To handle the change of the PSF with the observational offset during the analysis 
the \code{PSFMap} class is used. It stores the radial profile of the PSF
depending on the true energy and position on the sky. During the modeling
step in the analysis, the PSF profile for each model component is 
looked up at its current position and converted into a 3d convolution kernel
which is used for the prediction of counts from that model component.


\subsubsection{Energy Dispersion}
For \iacts, the energy resolution and bias, or sometimes called energy dispersion,
is typically parametrised in terms of the so-called
migration parameter ($\mu$), which is defined as the ratio between the
reconstructed energy and the true energy. By definition, the mean of this ratio is
close to unity for a small energy bias and its distribution can 
be typically described by a Gaussian. However, more complex
shapes are also common. The migration parameter is given at each offset angle and
reconstructed energy. The main sub-classes are the \code{EnergyDispersion2D} which is
designed to handle the raw instrument description, and the \code{EDispKernelMap},
which contains an energy disperion matrix per sky position. I.e., a 4-dimensional
sky map where at each position is associated to an energy dispersion matrix.
The energy dispersion matrix is a representation of the energy resolution
as a function of the true energy only and implemnted in \gammapy
by the sub-class \code{EDispKernel}.

\subsubsection{Instrumental Background}
The instrumental background rate can be represented in \gammapy as either a 2-dimensional
data structure (\code{Background2D}) of count rate normalised per steradians and energy at different
reconstructed energies and offset angles or as rate per steradians and energy, as a
function of reconstructed energy and detector coordinates (\code{Background3D}).
In the former, the background is expected to follow a radially symmetric shape,
while in the latter, it can be more complex.

Some example IRFs read from public data files and plotted with \gammapy are shown in Figure~\ref{fig:irfs}.

\subsection{gammapy.maps}
\label{ssec:gammapy-maps}
The \code{gammapy.maps} sub-package provides classes that represent data
structures associated with a set of coordinates or a region on a sphere. In
addition it allows to handle an arbitrary number of non-spatial data
dimensions, such as time or energy. It is organized around three types of
structures: geometries, sky maps and map axes, which inherit from the base
classes \code{Geom}, \code{Map} and \code{MapAxis} respectively.

The geometry object defines the pixelization scheme and map boundaries. It also
provides methods to transform between sky and pixel coordinates. Maps consist
of a geometry instance defining the coordinate system together with a
Numpy array containing the associated data. All map classes support a basic
set of arithmetic and boolean operations with  unit support, up- and downsampling
along extra axes, interpolation, resampling of extra axes, interactive visualisation
in notebooks and interpolation onto different geometries.

The \code{MapAxis} class provides a uniform application programming interface
(API) for axes representing
bins on any physical quantity, such as energy or angular offset.
Map axes can have physical units attached to them, as well as define
non-linearly spaced bins. The special case of time is covered by the
dedicated \code{TimeMapAxis}, which allows time bins to be non-contiguous,
as it is often the case with observational times. The generic
class \code{LabelMapAxis} allows the creation of axes for non-numeric entries.

To handle the spatial dimension the sub-package exposes a uniform API for
the FITS World Coordinate System (WCS), the HEALPix pixelization and
region-based data structure (see Figure~\ref{fig*:minted:gp_maps}).
This allows users to perform the same higher level operations on maps
independent of the underlying pixelisation scheme. The \code{gammapy.maps}
package is also used by external packages such as \textit{FermiPy}~\citep{Wood2017}

\begin{figure}
	\small
	\import{code-examples/generated/}{gp_maps}

	\caption{
        Using \code{gammapy.maps} to create a WCS, a HEALPix and a region
		based data structures. The initialisation parameters include
        consistently the positions of the center of the map, the pixel
        size, the extend of the map as well as the energy axis definition.
        The energy minimum and maximum values for the creation of the
        \code{MapAxis} object can be defined as strings also specifying the
        unit. Region definitions can be passed as strings following
        the DS9 region specifications \url{http://ds9.si.edu/doc/ref/region.html}.The output
		of the code example is shown in Figure~\ref{fig:code_example_gp_maps}.
        }
    \label{fig*:minted:gp_maps}
\end{figure}

\subsubsection{WCS Maps}
The FITS WCS pixelization supports a different number of projections to
represent celestial spherical coordinates in a regular rectangular grid.
\gammapy provides full support to data structures using this pixelization
scheme. For details see ~\cite{Calabretta2002}. This pixelisation
is typically used for smaller regions of interests, such as pointed
observations and is represented by a combination of the
\code{WcsGeom} and \code{WcsNDMap} class.


\subsubsection{HEALPix Maps}
This pixelization scheme ~\citep{Calabretta2002} provides a
subdivision of a sphere in which each pixel covers the same surface area as
every other pixel. As a consequence, however, pixel shapes are no longer
rectangular, or regular.
This pixelisation is typically used for all-sky data, such as data
from the \hawc or \fermi observatory. \gammapy natively supports
the multiscale definition of the HEALPix pixelisation and thus
allows for easy up and downsampling of the data. In addition to
the all-sky map, \gammapy also supports a local HEALPix
pixelisation where the size of the map is constrained to a given
radius.
For local neighbourhood operations, such as convolution \gammapy relies
on projecting the HEALPix data to a local tangential WCS grid.
This data structure is represented by the \code{HpxGeom} and \code{HpxNDMap}
classes. 


\subsubsection{Region Maps}
In this case, instead of a fine spatial grid
dividing a rectangular sky region, the spatial dimension is reduced to a single
bin with an arbitrary shape, describing a region in the sky with that same
shape. Typically, they are used together with a non-spatial dimension, for
example an energy axis, to represent how a quantity varies in that dimension
inside the corresponding region. To avoid the complexity of handling
spherical geometry for regions, the regions are projected onto the local
tangential plane using a WCS transform. This approach follows Astropy's \textit{Regions}
package \citep{AstropyRegions2022}, which is both used as an API to define regions
for users as well as handling the underlying geometric operations. Region based
maps are represented by the \code{RegionGeom} and \code{RegionNDMap} classes.


\subsection{gammapy.datasets}
\label{ssec:gammapy-datasets}
%
\begin{figure}
	\small
	\import{code-examples/generated/}{gp_datasets}
	\caption{
        Using \code{gammapy.datasets} to read existing reduced binned datasets.
        After the different datasets are read from disk they are collected into a
        common \code{Datasets} container. All dataset types have an associated
        name attribute to allow access by name later in the code. The
        environment variable \code{\$GAMMAPY\_DATA} is automtically resolved
        by \gammapy. The output
		of the code example is shown in Figure~\ref{fig:code_example_gp_datasets}.
    }
	\label{fig*:minted:gp_datasets}
\end{figure}
%
The \code{gammapy.datasets} subpackage contains classes to bundle
together binned data along with the associated models and likelihood function, which
provides an interface to the \code{Fit} class (Sec \ref{sssec:fit}) for
modeling and fitting purposes. Depending upon the type of analysis and the
associated statistic, different types of Datasets are supported. The \code{MapDataset} is
used for combined spectral and morphological (3D) fitting, while a 1D spectral 
fitting can be performed using the \code{SpectrumDataset}.
While the default fit statistics for both of these classes is the \emph{Cash}~\citep{Cash}
statistic, there are other classes which support
analyses where the background is measured from control regions, so called \enquote{off} obervations.
Those require the use of a different fit statistics, which takes into account the
uncertainty of the background measurement. This case is covered by the \code{MapDatasetOnOff}
and \code{SpectrumDatasetOnOff} classes, which use the \emph{WStat}~\citep{WStat} statistic.

The predicted counts are computed by convolution of the models with the associated
IRFs. Fitting of precomputed flux points is enabled through \code{FluxPointsDataset},
using \emph{$\chi^2$} statistics. Multiple datasets of same or different types can be
bundled together in \code{Datasets} (e.g., Figure \ref{fig*:minted:gp_datasets}),
where the likelihood from each constituent member is added, thus facilitating
joint fitting across different observations, and even different instruments
across different wavelengths. Datasets also provide functionalities for
manipulating reduced data, e.g. stacking, sub-grouping, plotting. Users can
also create their customized datasets for implementing modified likelihood
methods.

\subsection{gammapy.makers}
\label{ssec:gammapy-makers}
%
\begin{figure}
	\small
    \import{code-examples/generated/}{gp_makers}
	\caption{
        Using \code{gammapy.makers} to reduce DL3 level data into a
		\code{MapDataset}. All \code{Maker} classes represent 
		a step in the data reduction process. They take
        the configuration on initialisation of the class. They 
		also consistently define \code{.run()} methods, which take
		a dataset object and optionally an \code{Observation} 
		object. In this way, \code{Maker} classes can be chained
		to define more complex data reduction pipelines. The output
		of the code example is shown in Figure~\ref{fig:code_example_gp_makers}.
    }
	\label{fig*:minted:gp_makers}
\end{figure}
%
The \code{gammapy.makers} sub-package contains the various classes and functions required
to process and prepare \gammaray data from the DL3 to the DL4, representing the input for modeling and fitting.
First, events are binned and IRFs are interpolated and projected onto the chosen analysis
geometry. The end product of the data reduction process are a set of binned counts,
background exposure, psf and energy dispersion maps at the DL4 level. 
The \code{MapDatasetMaker} and \code{SpectrumDatasetMaker} are
responsible for this task for 3D and 1D analyses, respectively (see Figure~\ref{fig*:minted:gp_makers}).

Because the background models suffer from strong uncertainties it is required
to correct them from the data themselves. Several techniques are commonly used
in TeV \gammaray astronomy such as field-of-view background normalization or
background measurement in reflected regions, see~\cite{Berge07}.
Specific \code{Makers} such as the \code{FoVBackgroundMaker} or the
\code{ReflectedRegionsBackgroundMaker} are in charge of this process.
%% CN: I would add a single line explaining what each of them do

Finally, to limit other sources of systematic uncertainties, a data validity
domain is determined by the \code{SafeMaskMaker}. It can be used to limit the
extent of the field of view used, or to limit the energy range to, e.g., a domain
where the energy reconstruction bias is below a given value.


\subsection{gammapy.stats}
\label{ssec:gammapy-stats}
The \code{gammapy.stats} subpackage contains the fit statistics and the associated
statistical estimators commonly adopted in \gammaray astronomy. In
general, \gammaray observations count Poisson-distributed events at various sky
positions, and contain both signal and background events. Estimation of the
number of signal events is done through likelihood maximization. In \gammapy,
the fit statistics are Poisson log-likelihood functions normalized like
chi-squares, i.e., they follow the expression $2 \time\cdot \log{\mathcal{L}}$,
where $\mathcal{L}$ is
the likelihood function used. When the expected number of background events is known, the used statistic function 
 is the \emph{Cash} statistic
~\citep{Cash}. It is used by datasets using background templates such as the
\code{MapDataset}. When the number of background events is unknown and an OFF
measurement where only background events are expected is used, the statistic
function is \emph{WStat}. It is a profile log-likelihood statistic where the background
counts are marginalized parameters. It is used by datasets containing off
counts measurements such as the \code{SpectrumDatasetOnOff}, used for classical
spectral analysis.

To perform simple statistical estimations on counts measurements,
\code{CountsStatistic} classes encapsulate the aforementioned statistic functions to
measure excess counts and estimate the associated statistical significance,
errors and upper limits. They perform maximum likelihood ratio tests to
estimate significance (the square root of the statistic difference) and compute
likelihood profiles to measure errors and upper limits. The code example
\ref{codeexample:stats} shows how to compute the Li \& Ma
significance~\citep{LiMa} of a set of measurements.

\begin{figure}
	\small
	\import{code-examples/generated/}{gp_stats}
	\caption{
        Using \code{gammapy.stats} to compute statistical quantities
        such as excess, signficance and assymetric errors
        from counts based data. The data is passed on initialisation
        of the \code{WStatCountsStatistic} class. The quantities
        are the computed ON excess of the corresponding class
        attributes such as \code{stat.n\_sig} and \code{stat.sqrt\_ts}.
		The output of the code example is shown in Figure~\ref{fig:code_example_gp_stats}.
    }
	\label{fig*:minted:gp_stats}
\end{figure}
%% CN: the figure does not explain what alpha is

\subsection{gammapy.modeling}
\label{ssec:gammapy-modeling}
%
\code{gammapy.modeling} contains all the functionality related to modeling and fitting
data. This includes spectral, spatial and temporal model classes, as well as
the fit and parameter API.

\subsubsection{Models}
\label{sssec:models}
Source models in \gammapy (Eq.~\ref{eq:general_source_model}) are four-dimensional 
analytical models which support two spatial dimensions defined by the sky coordinates
$\ell, b$, an energy dimension $E$, and a time dimension $t$. To simplify the the definition of the
models, \gammapy uses a factorised representation of the total source
model:

\begin{equation}
    \phi(\ell, b, E, t) = F(E) \cdot G(\ell, b, E) \cdot H(t, E).
    \label{eq:source_model_dependency}
\end{equation}

The spectral component $F(E)$, described by the \code{SpectralModel} class, always
includes an \textit{amplitude} parameter to adjust the total flux of the model.
The spatial component $G(\ell, b, E)$, described by the \code{SpatialModel} class,
also depends on energy, in order to consider energy-dependent sources morphology.
Finally, the temporal component $H(t, E)$, described by the \code{TemporalModel}
class, also supports an energy dependency in order to consider spectral variations
of the model with time.

The models follow a naming scheme which contains the category as a suffix to
the class name. The spectral models include a special class of normed models,
named using the \code{NormSpectralModel} suffix.
These spectral models feature a dimension-less \textit{norm} parameter
instead of an \textit{amplitude} parameter with physical units. They
can be used as an energy-dependent multiplicative correction
factor to another spectral model. They are typically used for
adjusting template-based models, or, for example, to take into account
the absorbtion effect on \gammaray spectra caused by the extra-galactic background
light (EBL) (\code{EBLAbsorptionNormSpectralModel}). \gammapy supports a variety
of EBL absorption models, such as those from \cite{Franceschini2008}, \cite{Finke2010},
and \cite{Dominguez2011}.

The analytical spatial models are all normalized such as they integrate to
unity over the entire sky. The template spatial models may not, so in that special
case they have to be combined with a \code{NormSpectralModel}.

The \code{SkyModel} class represents the factorised model in Eq.~\ref{eq:source_model_dependency}
(the spatial and temporal components being optional).
A \code{SkyModel} object can represent the sum of several emission components:
either, for example, from multiple sources and from a diffuse emission, or from several spectral
components within the same source. To handle list of multiple \code{SkyModel} objects, \gammapy
implements a \code{Models} class.

%% CN: a link to the model gallery?
The model gallery provides a visual overview of the available models in
\gammapy. Most of the analytic models commonly used in \gammaray astronomy are
built-in. We also offer a wrapper to radiative models implemented in the Naima
package~\citep{naima}. The modeling framework can be easily extended with
user-defined models. For example, the radiaitve models of jetted Active Galactic Nuclei (AGN)
implemented in \agnpy, can be wrapped into \gammapy~\citep[see Section 3.5 of ][]{2022A&A...660A..18N}.

%% CN: here again, print() without output. Is it useful?
\begin{figure}
	\small
	\import{code-examples/generated/}{gp_models}
	\caption{Using \code{gammapy.modeling.models} to define a source model with a
    spectral, spatial and temporal component. For convenience the model
    parameters can be defined as strings with attached units. The spatial model
    takes an additional \code{frame} parameter which allow users to define
    the coordinate frame of the position of the model. The output
	of the code example is shown in Figure~\ref{fig:code_example_gp_models}.
    }
	\label{fig*:minted:gp_models}
\end{figure}

\subsubsection{Fit}
\label{sssec:fit}

%% CN: should we mention that the dataset also include the stat function?
%% and thus the fit interfaces only with the dataset
The \code{Fit} class provides methods to fit, i.e. optimise, model parameters and estimate
their errors and correlations. It interfaces with a \code{Datasets} object, which
in turn is connected to a \code{Models} object containing the model parameters in its
\code{Parameters} object. Models can be unique for a given dataset, or contribute to
multiple datasets, allowing e.g., to perform a joint fit to
multiple IACT datasets, or to jointly fit IACT and \fermi dataset. Many
examples are given in the tutorials.
%% CN: I would write "to perform a joint fit from multiple datasets from the same instruments
%% (e.g. collected with different configurations) or from multiple datasets from different gamma-ray
%% instruments (space- and ground-based)."

The \code{Fit} class provides a uniform interface to multiple fitting backends:
\begin{itemize}
	\setlength\itemsep{1em}
	\item \iminuit~\citep{iminuit}
	\item \code{scipy.optimize}~\citep{2020SciPy-NMeth}
	\item \sherpa~\citep{sherpa-2011, sherpa-2001}
\end{itemize}

Note that, for now, covariance matrix and errors are computed only for the fitting with 
\iminuit. However depending on
the problem other optimizers can better perform, so sometimes it can be useful
to run a pre-fit with alternative optimization methods. In future we plan to
extend the supported fitting backends, including for example solutions based on Markov chain Monte Carlo methods.
\footnote{a prototype is available in gammapy-recipes,
	\url{https://gammapy.github.io/gammapy-recipes/_build/html/notebooks/mcmc-sampling-emcee/mcmc_sampling.html}
}
%% CN: watchout with giving links to notebooks whose position might change or disappear in the future.

\subsection{gammapy.estimators}
\label{ssec:gammapy-estimators}
By fitting parametric models to the data, the total \gammaray
flux and its overall temporal, spectral and morphological components can be constrained.
In many cases though, it is useful to make a more detailed follow-up analysis by measuring the
flux in smaller spectral, temporal or spatial bins. This
possibly reveals more detailed emission features, which
are relevant for studying correlation with counterpart emissions.

The \code{gammapy.estimators} sub-module features methods to compute flux
points, light curves, flux maps and flux profiles from data.
The basic method for all these measurements is equivalent.
The initial fine bins of \code{MapDataset} are grouped into
larger bins. A multiplicative correction factor (the \textit{norm})
is applied to the best fit reference spectral
model and is fitted in the restricted data range, defined by the 
bin group only.

In addition to the best-fit flux \textit{norm}, all estimators compute
quantities corresponding to this flux. This includes:
the predicted number of total, signal and background
counts per flux bin; the total fit statistics
of the best fit model; the fit statistics of the
null hypothesis; and the difference between both,
the so-called TS value.
From the TS value the significance of the measured signal and associated flux
can be derived.

Optionally, the estimators can also compute more advanced quantities
such as asymmetric flux errors, flux upper limits
and one-dimensional profiles of the fit statistic,
which show how the likelihood functions varies with
the flux \textit{norm} parameter around the fit minimum.
This information is useful in inspecting the quality
of a fit, for which a parabolic
shape of the profile is asymptomatically expected at the best fit
values.

The base class of all algorithms is the \code{Estimator}  class.
The result of the flux point estimation are either stored in a
\code{FluxMaps} or \code{FluxPoints} object. Both objects
are based on an internal representation of the flux which is
independent of the Spectral Energy Distribution (SED) type. The flux is represented by a
the reference spectral model and an array of
normalisation values given in energy, time and spatial bins,
which factorises the deviation of the flux in a given
bin from the reference spectral model. This allows
user to conveniently transform between different
SED types. Table~\ref{tab:sed_types} shows an
overview and definitions of the supported SED types.
The actual flux values for each SED type are obtained
by multiplication of the \textit{norm} with the reference flux.

\begin{table*}
    \begin{center}
        \begin{tabular}{lll}
         \hline
         Type & Description & Unit Equivalency \\
         \hline
         dnde & Differential flux at a given energy & $\mathrm{TeV^{-1}~cm^{-2}~s^{-1}}$ \\
         e2dnde & Differential flux at a given energy  & $\mathrm{TeV~cm^{-2}~s^{-1}}$ \\
         flux & Integrated flux in a given energy range & $\mathrm{cm^{-2}~s^{-1}}$ \\
         eflux & Integrated energy flux in a given energy range & $\mathrm{erg~cm^{-2}~s^{-1}}$\\
         \hline
        \end{tabular}
    \end{center}
    \caption{Definition of the different SED types supported in \gammapy.}
    \label{tab:sed_types}
\end{table*}


Both result objects support the possibility to serialise
the data into multiple formats. This includes the
\gadf SED format \footnote{\url{https://gamma-astro-data-formats.readthedocs.io/en/latest/spectra/flux_points/index.html}},
FITS-based ND sky maps and other formats compatible with Astropy's \code{Table} and
\code{BinnedTimeSeries} data structures. This allows
users to further analyse the results with Astropy, for example using
standard algorithms for time analysis, such as
the Lomb-Scargle periodogram or the Bayesian
blocks. So far, \gammapy does not support unfolding of \gammaray spectra.
Methods for this will be implemented in a future version of \gammapy.
%% CN: reference for unfolding https://ui.adsabs.harvard.edu/abs/2007NIMPA.583..494A/abstract

The code example shown in Figure~\ref{fig*:minted:gp_estimators} shows how to use
the \code{TSMapEstimator} objects with a given input \code{MapDataset}.
In addition to the model, it allows to specify the energy
bins of the resulting flux and TS maps.


\begin{figure}
	\small
	\import{code-examples/generated/}{gp_estimators}
	\caption{Using the \code{TSMapEstimator} object from \code{gammapy.estimators} to compute a
        a flux, flux upper limits and TS map. The additional parameters \code{n\_sigma}
        and \code{n\_sigma\_ul} define the confidence levels (in multiples of the normal distribution width)
        of the flux error and and flux upper limit maps respectively. The output
		of the code example is shown in Figure~\ref{fig:code_example_gp_estimators}.
    }
    \label{fig*:minted:gp_estimators}
\end{figure}

\subsection{gammapy.analysis}
\label{ssec:gammapy-analysis}
The \code{gammapy.analysis} sub-module provides a high-level interface (HLI) for the most
common use cases identified in \gammaray analyses. The included classes and methods
 can be used in Python scripts, notebooks or as commands within \texttt{IPython}
sessions. The HLI can also be used to automatise
workflows driven by parameters declared in a configuration file in YAML format.
This way, a full analysis can be executed via a single command line taking the
configuration file as input.

The \code{Analysis} class has the responsibility of orchestrating of the workflow
defined in the configuration \code{AnalysisConfig} objects and triggering the execution of
the \code{AnalysisStep} classes that define the identified common use cases. These
steps include the following: observations selection with the \code{DataStore},  data
reduction, excess map computation, model fitting, flux points estimation, and
light curves production.
%% CN: I would not say that these are use cases, rather analysis or data reduction steps

\subsection{gammapy.visualization}
\label{ssec:gammapy-visualization}
The \code{gammapy.visualization} sub-package contains helper functions
for plotting and visualizing analysis results and \gammapy~data structures.
This includes for example the visualization of reflected background regions across
multiple observations or plotting large parameter correlation matrices of
\gammapy models. It also includes a helper class to split
wide field Galactic survey images across multiple panels to fit a standard
paper size.

The sub-package also provides \texttt{matplotlib} implementations of specific
colormaps for false color image representation. Those colormaps have
been historically used by larger collaborations in the very high-energy
domain (such as \milagro or \hess) as \enquote{trademark} colormaps.
While we explicitly discourage the use of those colormaps for publication
of new results, because they do not follow modern visualization
standards, such as linear brightness gradients and accessibility
for visually impaired people, we still consider the colormaps
useful for reproducibility of past results.

\subsection{gammapy.astro}
\label{ssec:gammapy-astro}
The \code{gammapy.astro} sub-package contains utility functions for studying physical
scenarios in high-energy astrophysics. The \code{gammapy.astro.darkmatter} module
computes the so called J-factors and the associated \gammaray spectra expected
from annihilation of dark matter in different channels according to the recipe
described in \cite{2011JCAP...03..051C}.

In the \code{gammapy.astro.source} sub-module, dedicated classes exist for modeling
galactic \gammaray sources according to simplified physical models, e.g. Supernova Remnant (SNR) evolution
models \citep{1950RSPSA.201..159T, 1999ApJS..120..299T}, evolution of Pulsar Wind Nebula (PWN) during the
free expansion phase \citep{2006ARA&A..44...17G} or computation
of physical parameters of a pulsar using a simplified dipole spin-down model.

In the \code{gammapy.astro.population} sub-module there are dedicated tools
for simulating synthetic populations based on physical models derived from
observational or theoretical considerations for different classes of Galactic
very high-energy \gammaray emitters: PWNe, SNRs \cite{1998ApJ...504..761C},
pulsars \cite{2006ApJ...643..332F, 2006MNRAS.372..777L, 2004A&A...422..545Y}
and \gammaray binaries.

While the present list of use cases is rather preliminary, this can be enriched
with time with by users and/or developers according to future needs.

\subsection{gammapy.catalog}
\label{ssec:gammapy-catalog}
Comprehensive source catalogs are increasingly being provided by many high-energy
 astrophysics experiments. The \code{gammapy.catalog} sub-packages
provides a convenient access to the most important \gammaray catalogs.
Catalogs are represented by the \code{SourceCatalog} object, which
contains the actual catalog as an Astropy \code{Table} object.
Objects in the catalog can be accesed by row index, name of the
object or any association or alias name listed in the catalog.

Sources are represented in \gammapy by the \code{SourceCatalogObject}
class, which has the responsibility to translate the information
contained in the catalog to other \gammapy objects. This includes
the spatial and spectral models of the source, flux points and
light curves (if available) for each individual object. This
module works independently from the rest of the package, and the required
catalogs are supplied in \code{GAMMAPY\_DATA} repository.
The overview of currenly supported catalogs, the corresponding
\gammapy classes and references are shown in Table~\ref{tab:catalogs}.
Newly released relevant catalogs will be added in future.

\begin{table*}[ht!]
    \begin{center}
        \begin{tabular}{llll}
         \hline
         Class Name & Shortcut & Description & Reference\\
         \hline
         \code{SourceCatalog3FGL} & \code{"3fgl"} & 3\textsuperscript{rd} catalog of \fermi sources & \cite{3FGL} \\
         \code{SourceCatalog4FGL} & \code{"4fgl"} & 4\textsuperscript{th} catalog of \fermi  sources & \cite{4FGL} \\
         \code{SourceCatalog2FHL} & \code{"2fhl"} & 2\textsuperscript{nd} catalog high-energy \fermi  sources & \cite{2FHL} \\
         \code{SourceCatalog3FHL} & \code{"3fhl"} & 3\textsuperscript{rd} catalog high-energy \fermi  sources & \cite{3FHL} \\
         \code{SourceCatalog2HWC} & \code{"2hwc"} & 2\textsuperscript{nd} catalog of \hawc sources & \cite{2HWC} \\
         \code{SourceCatalog3HWC} & \code{"3hwc"} & 3\textsuperscript{rd} catalog of \hawc sources & \cite{3HWC} \\
         \code{SourceCatalogHGPS} & \code{"hgps"} & \hess Galactic Plane Survey catalog & \cite{HGPS} \\
         \code{SourceCatalogGammaCat} & \code{"gammacat"} & Open source data collection & \cite{gamma-cat} \\
         \hline
         \end{tabular}
    \end{center}
    \caption{Overview of supported catalogs in \code{gammapy.catalog}.}
    \label{tab:catalogs}
\end{table*}

\begin{figure}
	\small
	\import{code-examples/generated/}{gp_catalogs}
	\caption{Using \code{gammapy.catalogs} to access the underlying model, flux points and
		light-curve from the \fermi 4FGL catalog for the blazar PKS 2155-304. The output
		of the code example is shown in Figure~\ref{fig:code_example_gp_catalogs}.
	}
	\label{fig*:minted:gp_catalogs}
\end{figure}

%\subsection{gammapy.utils}
%\label{ssec:gammapy-utils}
%Utility functions...


\section{Applications}
\label{sec:applications}
%% CN: I think we never explained what gammapy-data is and we should have at this point 
%% (it is used without mention also in the TSEstimator example).
\gammapy is currently used for a variety of analyses by different IACT
experiments and has already been employed in more than 60 scientific publications
\footnote{\href{https://ui.adsabs.harvard.edu/search/q=(\%20(citations(doi\%3A\%2210.1051\%2F0004-6361\%2F201834938\%22)\%20OR\%20citations(bibcode\%3A2017ICRC...35..766D))\%20AND\%20year\%3A2014-2023)&sort=date\%20desc\%2C\%20bibcode\%20desc&p_=0}{List on ADS}}.
In this section, we illustrate the capabilities of \gammapy by performing some standard
analysis cases commonly considered in \gammaray astronomy.
Beside reproducing standard methodologies, we illustrate the unique data combination
capabilites of Gammapy by presenting a multi-instrument analysis to date not possible within any
of the current instrument private software frameworks.
The examples shown are limited by the availability of public data,
with those employed being publicly available data collected within the \code{gammapy-data} repository.
We remark that, as long as the data are compliant with the GADF specifications,
and hence with Gammapy's data structures, there is no limitation on performing
analyses of data from a given instrument.


\subsection{1D Analysis}
\label{ssec:1d-analysis}
One of the most common analysis cases in \gammaray astronomy is measuring the
spectrum of a source in a given region defined on the sky, in conventional
astronomy also called \textit{aperture photometry}. The spectrum is typically measured
in two steps: first a parametric spectral model is fitted to the data and
secondly flux points are computed in a pre-defined set of energy bins. The
result of such an analysis performed on three simulated CTA observations is
shown in Fiure~\ref{fig:cta_galactic_center}. In this case the spectrum was
measured in a circular aperture centered on the Galactic Center, in
\gammaray~astronomy often called \enquote{on region}. For such analysis the users first
chooses a region of interest and energy binning, both defined by a
\code{RegionGeom}. In a second step, the events and the IRFs are binned
into maps of this geometry, by the \code{SpectrumDatasetMaker}. All the data and
reduced IRFs are bundled into a \code{SpectrumDataset}. To estimate
the expected background in the \enquote{on region} a \enquote{reflected regions} background
method was used~\cite{Berge07}, represented in \gammapy by the
\code{ReflectedRegionsBackgroundMaker} class. The resulting reflected regions are
illustrated for all three observations overlayed on the counts map in Figure~\ref{fig:cta_galactic_center}.
After reduction, the data were modelled using a forward-folding method and assuming
a point source with a power law spectral shape. The model was defined, using
the \code{SkyModel} class with a \code{PowerLawSpectralModel} spectral component only.
This model was then combined with the \code{SpectrumDataset}, which contains the reduced data 
and fitted using the and \code{Fit} class. Based on this best-fit model, the final flux points and corresponding
log-likelihood profiles are computed using the \code{FluxPointsEstimator}.

\begin{figure*}
	\centering
	\includegraphics[width=1.\textwidth]{figures/cta_galactic_center.pdf}
	\caption{
		Example of a one dimensional spectral analysis of the Galactic Center for three simulated CTA
		observations for the 1DC dataset. The left image shows the maps of counts with the signal
		region in white and background regions overlaid in different colors. The right image
		shows the resulting spectral points and their corresponding log-likelihood
		profiles.}
	\label{fig:cta_galactic_center}
\end{figure*}
%% CN: expand 1DC acronym and add a reference, here or in the section text

\subsection{3D Analysis}
\label{ssec:3d-analysis}
%
\begin{figure*}[t]
	\centering
	\includegraphics[width=1.\textwidth]{figures/cube_analysis.pdf}
	\caption{Example of a 3D analysis for simulated sources with point-like, Gaussian
		and shell-like morphologies. The simulation uses \textit{prod5} \irfs from \cta.
		The left image shows a significance map (using the \emph{Cash} statistics)
		where the three simulated sources can be seen. The middle figure shows another significance map,
		but this time after
		subtracting the best-fit model for each of the sources, which are displayed in
		black. The right figure shows the contribution of each source model to the
		circular region of radius 0.5\textdegree~drawn in the left image, together with
		the excess counts inside that region. }
	\label{fig:cube_analysis}
\end{figure*}
%
The 1D analysis approach is a powerful tool to measure the spectrum of an
isolated source. However, more complicated situations require a more careful
treatment. In a field of view containing several overlapping sources, the 1D
approach cannot disentangle the contribution of each source to the total flux in
the selected region. Sources with extended or complex morphology can result in
the measured flux being underestimated, and heavily dependent on the choice of
extraction region.

For such situations, a more complex approach is needed, the so-called 3D
analysis. The three relevant dimensions are the two spatial angular coordinates
and an energy axis. In this framework, a combined spatial and spectral model
(that is, a \code{SkyModel}, see Section~\ref{ssec:gammapy-modeling}) is fitted to the
sky maps that were previously derived from the data reduction step and bundled into a
\code{MapDataset} (see Sections~\ref{ssec:gammapy-makers} and~\ref{ssec:gammapy-datasets}).

A thorough description of the 3D analysis approach and multiple examples that
use \gammapy can be found in~\cite{Mohrmann2019}. Here we present a short
example to highlight some of its advantages.

Starting from the \irfs corresponding to the same three simulated \cta
observations used in Section~\ref{ssec:1d-analysis}, we can create a \code{MapDataset}
via the \code{MapDatasetMaker}. However, we will not use the simulated event lists
provided by \cta but instead, use the method \code{MapDataset.fake()} to simulate
measured counts from the combination of several \code{SkyModel} instances. In this
way, a DL4 dataset can directly be simulated. In particular we simulate:
\begin{enumerate}
    \item a point source located at (l=0\textdegree, b=0\textdegree) with a power law
	      spectral shape,
    \item an extended source with Gaussian morphology located at (l=0.4\textdegree,
	      b=0.15\textdegree) with $\sigma$=0.2\textdegree and a log parabola spectral
	      shape,
    \item a large shell-like structure centered on (l=0.06\textdegree,
	      b=0.6\textdegree) with a radius and width of 0.6\textdegree~and 0.3\textdegree~
	      respectively and a power law spectral shape.
\end{enumerate}

The position and sizes of the sources
have been selected so that their contributions overlap. This can be clearly
seen in the significance map shown in the left panel of
Figure~\ref{fig:cube_analysis}. This map was produced with the
\code{ExcessMapEstimator} (see Section~\ref{ssec:gammapy-estimators}) with a
correlation radius of 0.1\textdegree.

We can now fit the same model shapes to the simulated data and retrieve the
best-fit parameters. To check the model agreement, we compute the residual
significance map after removing the contribution from each model. This is done
again via the \code{ExcessMapEstimator}. As can be seen in the middle panel of
Figure~\ref{fig:cube_analysis}, there are no regions above or below 5$\sigma$,
meaning that the models describe the data sufficiently well.

As the example above shows, the 3D analysis allows to characterize the
morphology of the emission and fit it together with the spectral properties of
the source.  Among the advantages that this provides is the ability to
disentangle the contribution from overlapping sources to the same spatial
region. To highlight this, we define a circular \code{RegionGeom} of radius
0.5\textdegree~ centered around the position of the point source, which is drawn
in the left panel of Figure~\ref{fig:cube_analysis}. We can now compare the
measured excess counts integrated in that region to the expected relative
contribution from each of the three source models. The result can be seen in the right
panel of Figure~\ref{fig:cube_analysis}.

Note that all the models fitted also have a spectral component, from which flux
points can be derived in a similar way as described in~\ref{ssec:1d-analysis}.
%\end{figure*}%	\caption{Fermi-LAT TS map in two energy bands} \label{fig:fermi_ts_map}%	\includegraphics[width=1.\textwidth]{figures/fermi_ts_map.pdf}%Ref:~\citep{Stewart2009} \begin{figure*}[t] \centering%\todo{What to do with } Figure~\ref{fig:fermi_ts_map} ?%

\subsection{Temporal Analysis}
\label{ssec:temporal-analysis}
A common use case in most astrophysical scenarios is to study the temporal
variability of a source. The most basic way to do this is to construct a
light curve, i.e., the flux of a source in each given time bin. In \gammapy, this
is done by using the \code{LightCurveEstimator} that fits the normalisation of a
source in each time (and optionally energy) band per observation, keeping constant
other parameters.
For custom time binning, an observation needs to be split into finer time bins using
the \code{Observation.select\_time} method. Figure~\ref{fig:hess_lightcurve_pks}
shows the light curve of the blazar PKS~2155-304 in different energy bands as
observed by the \hess telescope during an exceptional flare on the night of
July 29 - 30, 2006~\cite{2009A&A...502..749A}. The data are publicly available 
as a part of the HESS-DL3-DR1~\cite{HESS_DR1}. Each observation is first split into 10 min smaller
observations, and spectra extracted for each of these within a 0.11\textdegree~radius
around the source. A \code{PowerLawSpectralModel} is fit to all the datasets, leading
to a reconstructed index of $3.54 \pm 0.02$. With this adjusted spectral model
the \code{LightCurveEstimator} runs directly for two energy bands, $0.5\,{\rm TeV} - 1.5\,{\rm TeV}$,
and $1.5\,{\rm TeV} - 20\,{\rm TeV}$, respectively.
%
\begin{figure*}[t]
    \sidecaption
	\includegraphics[width=0.6666\textwidth]{figures/hess_lightcurve_pks.pdf}
	\caption{
        Binned light curves in two different energy bands for the source
        PKS~2155-304 in two energy bands ($0.5\,{\rm TeV} - 1.5\,{\rm TeV}$, and $1.5\,{\rm TeV} - 20\,{\rm TeV}$)
        as observed by the \hess telescopes in 2006. The coloured markers
        show the flux points in the different energy bands. The horizontal
        error illustrates the width of the time bin of 10~min. The vertical
        error bars show the associated asymmetrical flux errors. The marker
        is set to the center of the time bin.
    }
    \label{fig:hess_lightcurve_pks}
\end{figure*}
%
The obtained flux points can be analytically modelled using the available or
user-implemented temporal models. Alternatively, instead of  extracting a
light curve, it is also possible to directly fit temporal models to the reduced
datasets. By associating an appropriate \code{SkyModel}, consisting of both temporal
and spectral components, or using custom temporal models with spectroscopic
variability, to each dataset, a joint fit across the datasets will directly
return the best fit temporal and spectral parameters.

\subsection{Multi-instrument Analysis}
\label{ssec:multi-instrument-analysis}
%
\begin{figure*}[t]
	\sidecaption
	\includegraphics[width=0.666\textwidth]{figures/multi_instrument_analysis.pdf}
	\caption{
        A multi-instrument spectral energy distribution (SED) and combined model fit
        of the Crab Nebula. The  colored markers show the flux points computed from
        the data of the different listed instruments. The horizontal error bar
        illustrates the width of the chosen energy band ($E_{Min}, E_{Max}$).
        The marker is set to the log-center energy of the band, that is
        defined by $\sqrt{E_{Min} \cdot E_{Max}}$. The vertical errors bars
        indicate the $1\sigma$ error of the measurement. The downward
        facing arrows indicate the value of $2\sigma$ upper flux limits
        for the given energy range. The black solid line shows the best
        fit model and the transparent band its $1\sigma$ error range.
		The band is to small be visible.
    }
	\label{fig:multi_instrument_analysis}
\end{figure*}
%
%% should we mention the joint-crab here or in the intro, where we talk about
%% DL3, multi-instrument analysis and so forth...
%% cite Laura's DL3 HAWC paper with the updated version of the joint-crab spectrum?
In this multi-instrument analysis example we showcase the capabilities of
\gammapy to perform a simultaneous likelihood fit incorporating data from
different instruments and at different levels of reduction. We estimate the
spectrum of the Crab Nebula combining data from the \fermi, \magic and \hawc
instruments.

The \fermi data is introduced at the data level DL4, and directly bundled in a
\code{MapDataset}. They ahve been prepared using the standard \textit{fermitools} \citep{Fermitools2019} and
selecting a region of $5^{\circ} \mathrm{x} 4^{\circ}$ around the
position of the Crab Nebula applying the same selection criteria of the 3FHL
catalog (7 years of data with energy from $10\,{\rm GeV}$ to $2\,{\rm TeV}$,
~\citealt{3FHL}).

The \magic data is included from the data level DL3. They consist of two
observations of $20\,{\rm min}$ each, chosen from the dataset used to estimate
the performance of the upgraded stereo system~\citep{magic_performance} and
already included in~\cite{joint_crab}. The observations were taken at small
zenith angles ($<30^{\circ}$) in wobble mode~\citep{fomin_1994}, with the
source sitting at an offset of $0.4^{\circ}$ from the FoV center. Their energy
range spans $80\,{\rm GeV} - 20\,{\rm TeV}$. They data reduction for the 1D analysis
is applied, and the data are reduced to a \code{SpectrumDataset} before being fitted.

\hawc data are directly provided as flux points (DL5 data level) and are read
via Gammapy's \code{FluxPoints} class. They were estimated in ~\cite{hawc_crab_2019}
with $2.5\,{\rm years}$ of data and span an energy range $300\,{\rm GeV} - 300\,{\rm TeV}$.

Combining the datasets in a \code{Datasets} list, \gammapy automatically generates
a likelihood including three different types of terms, two Poissonian likelihoods
for \fermi's \code{MapDataset} and MAGIC's \code{SpectrumDataset}, and a $\chi^2$
accounting for the \hawc flux points. For \fermi, a three-dimensional forward folding
of the sky model with the IRF is performed, in order to compute the predicted counts
in each sky-coordinate and energy bin. For \magic, a one-dimensional forward-folding
of the spectral model with the \irfs is performed to predict the counts in each estimated energy bin. A log
parabola is fitted to the almost five decades in energy $10\,{\rm GeV} - 300\,{\rm TeV}$.
%% CN: is the LP formula specified elsewhere?

The result of the joint fit is displayed in
Figure~\ref{fig:multi_instrument_analysis}. We remark that the objective of this
exercise is illustrative. We display the flexibility of \gammapy in
simultaneously fitting multi-instrument data even at different levels of
reduction, without aiming to provide a new measurement of the Crab Nebula
spectrum.

%% eventual part for physical modelling with naima

\subsection{Broadband SED Modeling}
\label{ssec:broadband-sed-modeling}
By combining \gammapy with astrophysical modelling codes, users can also fit
astrophysical spectral models to \gammaray data. In \gammaray
astronomy one typically observes two radiation production
mechanisms, the so-called hadronic and leptonic scenarios.
There are several Python packages that are able to model
the \gammaray emission, given a physical scenario. Among those
packages are Agnpy~\citep{agnpy}, Naima~\citep{naima}, Jetset~\citep{jetset}
and Gamera~\citep{gamera}.
Tyically those emission models predict broadband emission from
radio, up to the very high-energy \gammaray range.
By relying on the multiple dataset types in \gammapy those
data can be combined to constrain such a broadband emission model.
\gammapy provides a built-in \code{NaimaSpectralModel} that allows
users to wrap a given astrophysical emission model from the
Naima package and fit it directly to \gammaray data.

As an example of this application, we use the same multi-instrument
dataset described in the previous section and we fit it with an inverse
Compton model computed with Naima and wrapped in the \gammapy models
through the \code{NaimaSpectraModel} class. We describe the gamma-ray emission 
with an inverse Compton scenario, considering a log-parabolic
electron distribution that scatters: the synchrotron radiation produced by
the very same electrons; near and far infrared photon fields and the cosmic
microwave background (CMB). We adopted the prescription on the target
photon fields provided in the documentation of the \textit{Naima}
package\footnote{\url{https://naima.readthedocs.io/en/stable/examples.html\#crab-nebula-ssc-model}}.
The best-fit inverse Compton spectrum is represented with a red dashed line in
Figure~\ref{fig:multi_instrument_analysis}.

\subsection{Surveys, Catalogs, and Population Studies}
\label{ssec:surveys-catalogs-and-population-studies}

Sky surveys have a large potential for new source detections, and new phenomena
discovery in \gammaray astronomy. They also offer less selection bias to perform
source population studies over a large set of coherently detected and modelled objects.
Early versions of \gammapy were developed in parallel to the preparation of
the \hess Galactic plane survey catalog~\citep[HGPS, ][]{2018A&A...612A...1H} and
the associated PWN and SNR populations studies~\citep{2018A&A...612A...2H,
	2018A&A...612A...3H}. 

The increase in sensitivity and resolution provided by the new generation of
instruments scales up the number of detectable sources and the complexity of 
models needed to represent them accurately. As an example, if we compare the
results of the HGPS to the expectations from the \cta Galactic Plane survey
simulations, we jump from 78 sources detected by \hess to about 500 detectable by
CTA~\citep{2021arXiv210903729R}. This large increase in the amount of data to analyse
and increase in complexity of modelling scenarios, requires the high-level
analysis software to be both scalabale as well as performant. 

In short, the production of catalogs from \gammaray surveys can be divided in
four main steps: data reduction; object detection; model fitting and model
selection; associations and classification. All steps can either be done directly
with \gammapy or by relying on the seamless integration
of \gammapy with the scientific Python ecosystem. This allows to rely
on 3rd party functionality wherever needed.

The \iacts data reduction step is done in the same way than described in the 
previous sections but scaled up to few thousands of observations. The object
detection step typically consists in finding local maxima in the significance or TS maps,
computed by the \code{ExcessMapEstimator} or \code{TSMapEstimator} respectively.
 Further refinements can
include for example filtering and detection on these maps with techniques from
the \textit{Scikit-image} package~\citep{scikit-image}, and outlier detection from
the \textit{Scikit-learn} package~\citep{scikit-learn}. This allows e.g., to 
reduce the number of spurious detections at this stage using standard
classification algorithms and then speed up the next step
as less objects will have to be fitted simultaneously. During the modelling
step each object is alternatively fitted with different models in order to
determine their optimal parameters, and the best-candidate model. The
subpackage \code{gammapy.modeling.models} offers a large variety of choice, and the
possibility to add custom models.  Several spatial models (point-source, disk,
Gaussian...), and spectral models (power law, log parabola...) may be tested
for each object, so the complexity of the problem increases rapidly in regions
crowded with multiple extended sources. Finally an object is discarded if its
best-fit model is not significantly preferred over the null hypothesis (no
source) comparing the difference in log likelihood between these two
hypotheses.

For the association and classification step, that is tightly connected to the
population studies, we can easily compare the fitted models to the set of
existing \gammaray catalogs available in \code{gammapy.catalog}. Further
multi-wavelength cross-matches are usually required to characterize the
sources. This an e.g. easily be achieved by relying on coordinate
handling from Astropy or affiliated packages such as \textit{AstroML}~\citep{astroML}
or \textit{Astroquery}~\citep{astroquery}.

Studies performed on simulations not only offer a first glimpse on what could
be the sky seen by CTA (according to our current knowledge on source
populations), but also give us the opportunity to test the software on complex
use cases\footnote{Note that the CTA-GPS simulations were performed with the
	\textit{ctools} package~\citep{2016A&A...593A...1K} and analysed with both
	\textit{ctools} and \textit{gammapy} packages in order to cross-validate
	them.}. So we can  improve performances, optimize our analyses strategies, and
identify the needs in term of parallelisation to process the large datasets
provided by the surveys.



\section{\gammapy Project} \label{sec:gammapy-project}

In this section, we provide an overview of the organization of the \gammapy
project. We briefly describe the main roles and responsibilities within the
team, as well as the technical infrastructure designed to facilitate the
development and maintenance of \gammapy as a high-quality software. We use
common tools and services for software development of Python open-source
projects, code review, testing, package distribution and user support, with a
customized solution for a versioned and thoroughly-tested documentation in the form
of user-friendly playable tutorials. This section concludes with an outlook on
the roadmap for future directions.

\subsection{Organizational Structure}
\label{ssec:organizational-structure}

\gammapy is an international open-source project with a broad
developer base and contributions and commitments from mutiple groups and
leading institutes in the very high-energy astrophysics
domain\footnote{\url{https://gammapy.org/team.html}}. The main development
roadmaps are discussed and validated by a \textit{Coordination Committee}, composed of
representatives of the main contributing institutions and observatories.
This committee is
chaired by a \textit{Project Manager} and his deputy while two \textit{Lead Developers} manage
the development strategy and organise technical activities. This
institutionally-driven organisation, the permanent staff and commitment of
supporting institutes ensure the continuity of the executive teams. A core team
of developers from the contributing institutions is in charge of the regular
development, which benefits from regular contributions of the community at
large.

\subsection{Technical Infrastructure}
\label{ssec:technical-infrastructure}

\gammapy follows an open-source and open-contribution development model based on
the cloud repository service \github. A \github organization
\textit{gammapy}\footnote{\url{https://github.com/gammapy}} hosts different
repositories related with the project. The software codebase may be found in
the \textit{gammapy} repository (see
Figure~\ref{fig:codestats:lang} for code lines statistics). We make extensive
use of the pull request system to discuss and review code contributions.

% \begin{table}
% 	\import{tables/generated/}{codestats}
% 	\caption{
%         Overview of used programming languages and distribution of code across the different file
%         categories in the \gammapy code base. The most right column list the total number of lines
%         in a file. The \textit{comment} column lists the number of comments in the files
%         (including method and class docstrings), the \textit{blank} column lists the
%         number of blank lines in the file. The uppermost rows distinguish between
%         code that implements actual functionality and code the implements tests.
%         All documentataion in \gammapy is implemented as dosctrings, \text{reStructuredText} (RST)
%         files or notebooks.
%     }
% 	\label{table:codestats:data}
% \end{table}

\begin{figure}[t]
	\centering
	\includegraphics[width=0.5\textwidth]{figures/codestats.pdf}
	\caption{
		Overview of used programming languages and distribution of code across the different file
        categories in the \gammapy code base. The total number of lines is $\approx 50 000$.
    }
	\label{fig:codestats:lang}
\end{figure}

Several automated tasks are set as \github
actions\footnote{\url{https://github.com/features/actions}}, blocking the
processes and alerting developers when fails occur. This is the case of the
continuous integration workflow, which monitors the execution of the test coverage
suite\footnote{\url{https://pytest.org}} using datasets from the
\textit{gammapy-data} repository\footnote{\url{https://github.com/gammapy/gammapy-data}}.
Tests scan not only the codebase, but also the
code snippets present in docstrings of the scripts and in the RST documentation
files, as well as in the tutorials provided in the form of Jupyter notebooks.

Other automated tasks, executing in the
\textit{gammapy-benchmarks}\footnote{\url{https://github.com/gammapy/gammapy-benchmarks}} repository,
are responsible for numerical validation tests and benchmarks monitoring. Also,
tasks related with the release process are partially automated, and every
contribution to the codebase repository triggers the documentation building and
publishing workflow within the
\textit{gammapy-docs} repository\footnote{\url{https://github.com/gammapy/gammapy-docs}}
(see Sec.~\ref{ssec:software-distribution} and Sec.~\ref{ssec:documentation-and-user-support}).

This small ecosystem of interconnected up-to-date repositories, automated tasks
and alerts, is just a part of a bigger set of \github repositories, where most
of them are related with the project but not necessary for the development of
the software (i.e., project webpage, complementary high-energy astrophysics
object catalogs, coding sprints and weekly developer calls minutes,
contributions to conferences, other digital assets, etc.) Finally, third-party
services for code quality metrics are also set and may be found as status
shields in the codebase repository.

\subsection{Software Distribution}
\label{ssec:software-distribution}
\gammapy is distributed for Linux, Windows and Mac environments, and installed
in the usual way for Python packages. Each stable release is uploaded to the
Python package index\footnote{\url{https://pypi.org}} and as a binary package
to the \textit{conda-forge} and \textit{astropy} Anaconda
repository\footnote{\url{https://anaconda.org/anaconda/repo}} channels. At this
time, \gammapy is also available as a Debian Linux
package\footnote{\url{https://packages.debian.org/sid/python3-gammapy}}. We
recommend installing the software using the \textit{conda} installation process
with an environment definition file that we provide, so to work within a
virtual isolated environment with additional useful packages and ensure
reproducibility.

\gammapy is indexed in Astronomy Source Code
Library\footnote{\url{https://ascl.net/1711.014}} and
Zenodo\footnote{\url{https://doi.org/10.5281/zenodo.4701488}} digital libraries for
software. The Zenodo record is synchronised with the codebase \github repository
so that every release triggers the update of the versioned record. In addition,
the next release of \gammapy will be added to the Open-source scientific
Software and Service Repository\footnote{\url{https://projectescape.eu/ossr}}
and indexed in the European Open Science Cloud
catalog \footnote{\url{https://eosc-portal.eu}}.

In addition \gammapy is also listed in the \textit{SoftWare
Heritage}~\footnote{\url{https://softwareheritage.org}} (SWH) archive~\cite{DiCosmo2020}.
The archive collects, preserves, and shares the source code of publicly available software.
SWH automatically scans open software repositories, like e.g. GitHub, and projects are archived in SWH by the
means of SoftWare Heritage persistent IDentifiers (SWHID), that are guaranteed to remain stable (persistent)
over time. The French open publication archive, HAL~\footnote{\url{https://hal.archives-ouvertes.fr}},
is using the \gammapy SWHIDs to register the releases as scientific
products~\footnote{\url{https://hal.science/hal-03885031v1}} of open science.

\subsection{Documentation and User-support}
\label{ssec:documentation-and-user-support}
\gammapy provides its user community with a tested and versioned up-to-date
online
documentation\footnote{\url{https://docs.gammapy.org}}~\citep{2019ASPC..523..357B}
built with Sphinx\footnote{\url{https://www.sphinx-doc.org}} scanning the
codebase Python scripts, as well as a set of RST files and Jupyter notebooks.
The documentation includes a user guide, a set of executable
tutorials, and a reference to the API automatically extracted from the code and
docstrings. The \gammapy code snippets present in the documentation are tested
in different environments using our continuous integration (CI) workflow based
on \github actions.

The Jupyter notebooks tutorials are generated using the sphinx-gallery
package \citep{sphinx-gallery}.
The resulting web published tutorials also provide links to playground spaces in
\textit{myBinder}~\citep{project_jupyter-proc-scipy-2018}, where they may be executed
on-line in versioned virtual environments hosted in the myBinder
infrastructure. Users may also play with the tutorials locally in their
laptops. They can download a specific version of the tutorials together with
the associated datasets needed and the specific conda computing environment,
using the \textit{gammapy download} command.

We have also set up a solution for users to share recipes as Jupyter notebooks
that do not fit in the \gammapy core documentation but which may be relevant as
specific use cases. Contributions happen via pull requests to the
\textit{gammapy-recipes} \github repository and merged after a short review. All
notebooks in the repository are tested and published in the \gammapy recipes
webpage\footnote{\url{https://gammapy.github.io/gammapy-recipes}} automatically
using \github actions.

A growing community of users is gathering around the Slack
messaging\footnote{\url{https://gammapy.slack.com}} and \github
discussions\footnote{\url{https://github.com/gammapy/gammapy/discussions}}
support forums, providing valuable feedback on the \gammapy functionalities,
interface and documentation. Other communication channels have been set like
mailing lists, a Twitter account\footnote{\url{https://twitter.com/gammapyST}},
regular public coding sprint meetings, hands-on session within collaborations,
weekly development meetings, etc.

\subsection{Proposals for Improving \gammapy}
\label{ssec:pigs}
An important part of \gammapy's development organisation is the support
for \textit{Proposals for improving \gammapy}(PIG). This system is very much
inspired by Python's PEP\footnote{\url{https://peps.python.org/pep-0001/}}
and Astropy's APE \citep{greenfield_perry_2013} system.
PIG are self-contained documents which outline a set of larger
changes to the \gammapy code base. This includes larger feature additions,
code and package restructuring and maintenance as well as changes related
to the organisational structure of the \gammapy project. PIGs can be proposed
by any person in or outside the project and by multiple authors. They
are presented to the \gammapy developer community in a pull request
on \github and the undergo a review phase in which changes and
improvements to the document are proposed and implemented. Once the PIG
document is in a final state it is presented to the \gammapy
coordination committee, which takes the final decision on the
acceptance or rejection of the proposal. Once accepted, the proposed
change are implemented by \gammapy developers in a series of
individual contributions via pull requests. A list of all proposed
PIG documents is available in the \gammapy online documentation
\footnote{\url{https://docs.gammapy.org/dev/development/pigs/index.html}}.

A special category of PIGs are long-term \textit{roadmaps}. To develop a common
vision for all \gammapy project members on the future of the
project, the main goals regarding planned features, maintenance and
project organisation are written up as an overview and presented to the
\gammapy community for discussion. The review and acceptance process
follows the normal PIG guidelines. Typically roadmaps are written
to outline and agree on a common vision for the next long term
support release of \gammapy.

\subsection{Release Cycle, Versioning, and Long-term Support}
\label{ssec:release-cycle}
With the first long term support (LTS) release v1.0, the \gammapy project
enters a new development phase. The development will change from
quick feature-driven development to more stable maintenance
and user support driven developement. After v1.0 we foresee
a developement cycle with major, minor and bugfix releases;
basically following the development cycle of the Astropy
project. Thus we expect a major LTS release approximately
every two years, minor releases are planned every 6~months,
while bug-fix releases will happen as needed. While
bug-fix releases will not introduce API-breaking changes,
we will work with a deprecation system for minor releases.
API-breaking changes will be announced to user by runtime
warnings first and then implemented in the subsequent
minor release. We consider this approach as a fair
compromise between the interests of users in a stable
package and the interest of developers to improve
and develop \gammapy in future. The development cycle is described
in more detail in PIG 23 \citep{gammapy_pig_23}.

\section{Reproducibility}
\label{sec:reproducibility}
One of the most important goals of the \gammapy project is to support open and
reproducible science results. Thus we decided to write this manuscript
openly and publish the Latex source code along with the associated
Python scripts to create the figures
in an open repository~\footnote{\url{https://github.com/gammapy/gammapy-v1.0-paper}}.
This \github repository also documents the history of the creation
and evolution of the manuscript with time. To simplify the reproducibility
of this manuscript including figures and text, we relied on the tool
\textit{showyourwork}~\citep{Luger2021}. This tool coordinates the building
process and both software and data dependencies, such that the complete
manuscript can be reproduced with a single \code{make} command, after
downloading the source repository. For this we provide
detailed instructions online\footnote{\url{https://github.com/gammapy/gammapy-v1.0-paper/blob/main/README.md}}.
Almost all figures in this manuscript provide a link
to a Python script, that was used to produce it. This means all
example analyses presented in Sec.\ref{sec:applications} link to
actually working Python source code.


\section{Summary and Outlook}
\label{sec:summary-and-outlook}
%
In this manuscript we presented the first LTS version of \gammapy.
\gammapy is a Python package for \gammaray astronomy, which relies on the
scientific Python ecosystem, including Numpy and Scipy and Astropy as
main dependencies. It also holds the status of an Astropy affiliated
package. It supports high-level analysis of astronomical \gammaray
data from intermediate level data formats, such as the FITS based
\gadf. Starting from lists of \gammaray events and corresponding description
of the instrument response users can reduce and project the data
to WCS, HEALPix and region based data structures. The reduced data is bundled
into datasets, which serve as a basis for Poisson maximum likelihood
modelling of the data. For this purpose \gammapy provides a wide selection
of built-in spectral, spatial and temporal models, as well as unified
fitting interface with connection to multiple optimization backends.

With the v1.0 milestone the \gammapy project enters a new development
phase. Future work will not only include maintenance of the v1.0 release,
but also parallel development of new features, improved API and data
model support. While v1.0 provides all the features required for
standard and advanced astronomical \gammaray data analysis,
we already identified specific improvements to be considered in the
roadmap for a future v2.0 release. This includes the support for
scalable analyses via distributed computing. This will allow
users to scale an analysis from a few observations to multiple
hundreds of observations as expected by deep surveys of the CTA
observatory. In addition the high-level interface
of \gammapy is planned to be developed into a fully configurable
API design. This will allow users to define arbitrary complex analysis
scenarios as YAML files and even extend their workflows by user defined
analysis steps via a registry system. Another important topic will
be to improve the support of handling metadata for data structures
and provenance information to track the history of the data reduction
process from the DL3 to the highest DL5/DL6 data levels.

Around the core Python package a large diverse community of
users and contributors has developed. With regular developer meetings,
coding sprints and in-person user tutorials at relevant conferences
and collaboration meetings, the community has constantly grown.
So far \gammapy has seen ~80 contributors from ~10 different countries.
With typically ~10 regular contributors at any given time of the
project, the code base has constantly grown its range of features
and improved its code quality. With \gammapy being officially selected
in 2021 as the base library for the future science tools for CTA
\footnote{\href{https://www.cta-observatory.org/ctao-adopts-the-gammapy-software-package-for-science-analysis/}{CTAO Press Release}},
we expect the community to grow
even further, providing a stable perspective for further usage,
development and maintenance of the project. Besides the future use
by the CTA community \gammapy has already
been used for analysis of data from the \hess, \magic, \astri and \veritas instruments.

While \gammapy was mainly developed for the science community around
IACT instruments, the internal data model and software design are general
enough to be applied to other \gammaray instruments as well.
The use of \gammapy for the analysis of data from the High Altitude
Water Cherenkov Observatory (HAWC) has been successfully
demonstrated by \cite{Olivera2022}. This makes \gammapy
a viable choice for the base library for the science tools
of the future Southern Widefield Gamma Ray Observatory
(SWGO) and use with data from Large High Altitude Air Shower Observatory (LHAASO) as well. \gammapy
has the potential to further unify the community
of \gammaray astronomers, by sharing common tools and
a common vision of open and reproducible science for the future.

\begin{acknowledgements}

	We would like to thank the \texttt{Numpy}, \texttt{Scipy}, \texttt{IPython} and
	\texttt{Matplotlib} communities for providing their packages which are
	invaluable to the development of Gammapy. We thank the \github team for
	providing us with an excellent free development platform. We also are grateful
	to Read the Docs (\ReadthedocsUrl), and Travis (\TravisUrl) for providing free
	documentation hosting and testing respectively. Α special acknowledgment has to be given
	to our first Lead Developer of \gammapy, Christoph Deil. Finally, we would like to thank
	all the \gammapy users that have provided feedback and submitted bug reports.
	J.E.~Ruiz acknowledges financial support from the State Agency for Research of
	the Spanish MCIU through the \textit{Center of Excellence Severo Ochoa} award to the
	Instituto de Astrof\'isica de Andaluc\'ia (SEV-2017-0709). L.~Giunti acknowledges
	financial support from the Agence Nationale de la Recherche (ANR-17-CE31-0014).

\end{acknowledgements}


% Back matter
\bibliographystyle{aa}
\bibliography{bib.bib}

\appendix
\section{Code Examples Output}

\begin{figure}[!ht]
	\small
	\import{code-examples/generated-output/}{gp_data}
	\caption{Output from the code example shown in Figure~\ref{fig*:minted:gp_data}}
	\label{fig:code_example_gp_data}
\end{figure}


\begin{figure}[!ht]
	\small
	\import{code-examples/generated-output/}{gp_datasets}
	\caption{Output from the code example shown in Figure~\ref{fig*:minted:gp_datasets}}
	\label{fig:code_example_gp_datasets}
\end{figure}

\begin{figure}[!ht]
	\small
	\import{code-examples/generated-output/}{gp_maps}
	\caption{Output from the code example shown in Figure~\ref{fig*:minted:gp_maps}}
	\label{fig:code_example_gp_maps}
\end{figure}


\begin{figure}[!ht]
	\small
	\import{code-examples/generated-output/}{gp_stats}
	\caption{Output from the code example shown in Figure~\ref{fig*:minted:gp_stats}}
	\label{fig:code_example_gp_stats}
\end{figure}


\begin{figure*}[!ht]
	\centering
	\includegraphics[width=1.\textwidth]{figures/gp_makers.pdf}
	\caption{Output from the code example shown in Figure~\ref{fig*:minted:gp_makers}}
	\label{fig:code_example_gp_makers}
\end{figure*}


\begin{figure*}[!ht]
	\centering
	\includegraphics[width=1.\textwidth]{figures/gp_estimators.pdf}
	\caption{Output from the code example shown in Figure~\ref{fig*:minted:gp_estimators}}
	\label{fig:code_example_gp_estimators}
\end{figure*}


\begin{figure*}[!ht]
	\centering
	\includegraphics[width=1.\textwidth]{figures/gp_catalogs.pdf}
	\caption{Output from the code example shown in Figure~\ref{fig*:minted:gp_catalogs}}
	\label{fig:code_example_gp_catalogs}
\end{figure*}

\begin{figure}[!ht]
	\small
	\import{code-examples/generated-output/}{gp_models}
	\caption{Output from the code example shown in Figure~\ref{fig*:minted:gp_models}}
	\label{fig:code_example_gp_models}
\end{figure}


\end{document}
